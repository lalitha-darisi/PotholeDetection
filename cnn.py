# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y34ADzRtzswVSBpJ9egzLwQq7wuied4w
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation, ColorJitter
import os
from PIL import Image
import random
import numpy as np

# --- Set Seed for Reproducibility --- #
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# --- Data Loading and Preprocessing --- #
class PotholeDataset(Dataset):
    def __init__(self, pothole_dir, normal_dir, transform=None):
        self.images = []
        self.labels = []
        self.transform = transform

        for image_file in os.listdir(pothole_dir):
            image_path = os.path.join(pothole_dir, image_file)
            image = Image.open(image_path).convert('L').resize((64, 64))
            self.images.append(image)
            self.labels.append(1)  # Label 1 for pothole

        for image_file in os.listdir(normal_dir):
            image_path = os.path.join(normal_dir, image_file)
            image = Image.open(image_path).convert('L').resize((64, 64))
            self.images.append(image)
            self.labels.append(0)  # Label 0 for normal

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Data directories
pothole_dir = r'newdata\Dataset\Pothole'
normal_dir = r'newdata\Dataset\Normal'

# Apply transformations
transform = Compose([
    RandomHorizontalFlip(),
    RandomRotation(15),
    ColorJitter(brightness=0.3, contrast=0.3),
    ToTensor(),
    Normalize(mean=[0.5], std=[0.5])
])

dataset = PotholeDataset(pothole_dir, normal_dir, transform=transform)

# Split dataset
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# --- Define Enhanced CNN Model --- #
class EnhancedCNN(nn.Module):
    def __init__(self):
        super(EnhancedCNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2)  # Output: 32x32
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2)  # Output: 16x16
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2)  # Output: 8x8
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2)  # Output: 4x4
        )
        self.fc1 = nn.Sequential(
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        self.fc2 = nn.Linear(512, 1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = x.view(-1, 256 * 4 * 4)  # Flatten
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Instantiate model
model = EnhancedCNN().to(device)

# Loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# --- Training Loop --- #
num_epochs = 30

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.float().view(-1, 1).to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + Backward + Optimize
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Step the scheduler
    scheduler.step()

    # Validation loss
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.float().view(-1, 1).to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

    val_loss /= len(test_loader)
    running_loss /= len(train_loader)
    print(f"Epoch {(epoch + 1)}/{num_epochs}, Training Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}")
# Save the model's state_dict
torch.save(model.state_dict(), 'model.pth')
print("Model saved successfully.")
# --- Evaluate Model Accuracy --- #
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        predictions = (torch.sigmoid(outputs) > 0.5).float()
        correct += (predictions.view(-1) == labels).sum().item()
        total += labels.size(0)

accuracy = 100 * correct / total
print(f"Accuracy: {accuracy:.2f}%")
